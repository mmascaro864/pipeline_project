{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6844b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3208f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    sql_url = 'sqlite:///'+ database_filepath\n",
    "    table = os.path.basename(database_filepath)\n",
    "    engine = create_engine(sql_url)\n",
    "    df = pd.read_sql_table(table, engine)\n",
    "    \n",
    "    X = df['message'].values\n",
    "    y = df[df.columns[4:]].values\n",
    "    \n",
    "    #category_names = y.columns\n",
    "    category_names = list(df.columns)[4:]\n",
    "    \n",
    "    return X, y, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb07c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_filepath = 'data/DisasterResponse.db'\n",
    "\n",
    "X, y, category_names = load_data(database_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b443be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26216,) (26216, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5697fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8169f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "        build_model:\n",
    "            - Build Natural Language Processing ML pipeline model\n",
    "            - Processes and transforms text messages and applies a classifier\n",
    "\n",
    "            In:\n",
    "                - None\n",
    "\n",
    "            Out: pipeline model\n",
    "    '''\n",
    "    pipeline = Pipeline([\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "            ])),\n",
    "        ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82eba381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('text_pipeline',\n",
       "                 Pipeline(steps=[('vect',\n",
       "                                  CountVectorizer(tokenizer=<function tokenize at 0x7fef67b2f430>)),\n",
       "                                 ('tfidf', TfidfTransformer()),\n",
       "                                 ('clf',\n",
       "                                  MultiOutputClassifier(estimator=RandomForestClassifier()))]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# train classifier\n",
    "model = build_model()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d46950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, category_names):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # model accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    overall_accuracy = (y_pred == y_test).mean().mean()\n",
    "    print('Model accuracy by category:\\n {}'.format(accuracy))\n",
    "    print('\\nOverall model accuracy: {}'.format(overall_accuracy))\n",
    "\n",
    "    # classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=category_names, output_dict = True)\n",
    "    df_class_rpt = pd.DataFrame(report).transpose()\n",
    "    print(df_class_rpt)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3bf3b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Model accuracy by category:\n",
      " 0.9446932548763213\n",
      "\n",
      "Overall model accuracy: 0.9446932548763213\n",
      "                        precision    recall  f1-score  support\n",
      "related                  0.814196  0.973540  0.886767   4006.0\n",
      "request                  0.879271  0.451991  0.597061    854.0\n",
      "offer                    0.000000  0.000000  0.000000     22.0\n",
      "aid_related              0.782028  0.595745  0.676293   2162.0\n",
      "medical_help             0.600000  0.046053  0.085540    456.0\n",
      "medical_products         0.850000  0.062044  0.115646    274.0\n",
      "search_and_rescue        0.428571  0.024000  0.045455    125.0\n",
      "security                 0.500000  0.011364  0.022222     88.0\n",
      "military                 0.500000  0.029586  0.055866    169.0\n",
      "water                    0.895833  0.269592  0.414458    319.0\n",
      "food                     0.840741  0.411978  0.552984    551.0\n",
      "shelter                  0.871795  0.227679  0.361062    448.0\n",
      "clothing                 0.666667  0.024390  0.047059     82.0\n",
      "money                    1.000000  0.007812  0.015504    128.0\n",
      "missing_people           0.000000  0.000000  0.000000     73.0\n",
      "refugees                 0.375000  0.017341  0.033149    173.0\n",
      "death                    0.925926  0.104167  0.187266    240.0\n",
      "other_aid                0.454545  0.007278  0.014327    687.0\n",
      "infrastructure_related   0.333333  0.002933  0.005814    341.0\n",
      "transport                0.863636  0.071429  0.131944    266.0\n",
      "buildings                1.000000  0.056818  0.107527    264.0\n",
      "electricity              0.750000  0.027778  0.053571    108.0\n",
      "tools                    0.000000  0.000000  0.000000     28.0\n",
      "hospitals                0.000000  0.000000  0.000000     58.0\n",
      "shops                    0.000000  0.000000  0.000000     21.0\n",
      "aid_centers              0.000000  0.000000  0.000000     67.0\n",
      "other_infrastructure     0.333333  0.004464  0.008811    224.0\n",
      "weather_related          0.893617  0.622222  0.733624   1485.0\n",
      "floods                   0.926667  0.319540  0.475214    435.0\n",
      "storm                    0.796875  0.418033  0.548387    488.0\n",
      "fire                     0.000000  0.000000  0.000000     51.0\n",
      "earthquake               0.915094  0.760784  0.830835    510.0\n",
      "cold                     0.846154  0.104762  0.186441    105.0\n",
      "other_weather            0.857143  0.021127  0.041237    284.0\n",
      "direct_report            0.833729  0.354188  0.497167    991.0\n",
      "micro avg                0.826962  0.490502  0.615769  16583.0\n",
      "macro avg                0.592404  0.172247  0.220892  16583.0\n",
      "weighted avg             0.773127  0.490502  0.534398  16583.0\n",
      "samples avg              0.699451  0.470247  0.514531  16583.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating model...')\n",
    "evaluate_model(model, X_test, y_test, category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc147d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
